# 문서 4. 고급 패턴 II: 외부 시스템 & 인프라 (S2.4-S2.6)

> 이 문서는 **Stage 2 후반부 + Stage 3 (인프라)** 용이다.
> 전제:
>
> * Stage 1-2 전반부 (S2.0-S2.3) 완료
> * Spring Boot 핵심 패턴 (Controller/Service/Repository) 숙지
> * 이제 **Elasticsearch 검색**, **Kafka 이벤트 스트리밍**, **프로덕션 인프라**를 경험할 준비가 된 상태

---

## 0. 전제 / 목표

### 전제

* S2.0-S2.3 완료 (Spring Boot 기초 + RBAC + 배치/캐시)
* Docker Compose 사용 가능 (Elasticsearch, Kafka 로컬 실행용)
* 비동기 처리 개념 이해

### 이 문서의 목표

1. **Elasticsearch** 전문 검색 구현 (S2.4)
2. **Kafka** Producer/Consumer 패턴 (S2.5)
3. **Event-driven architecture** 경험
4. **프로덕션 인프라** 구축 (S2.6): PostgreSQL, Redis, Docker

### 이 문서가 다루는 범위

* **S2.4**: Elasticsearch Search
* **S2.5**: Kafka Async Events
* **S2.6**: Docker, PostgreSQL, Redis 인프라

---

## 1. S2.4: Elasticsearch Search

### 1.1 목표

* RDB와 분리된 검색 인덱스 구축
* 전문 검색 API 구현
* DB-ES 동기화 전략

### 1.2 Docker Compose (Elasticsearch)

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  elasticsearch_data:
```

**실행**:

```bash
docker-compose up -d elasticsearch
curl http://localhost:9200
# {"name":"...","cluster_name":"docker-cluster",...}
```

### 1.3 Elasticsearch 클라이언트 설치

**build.gradle**:

```gradle
dependencies {
    implementation 'co.elastic.clients:elasticsearch-java:8.11.0'
    implementation 'com.fasterxml.jackson.core:jackson-databind'
}
```

**ElasticsearchConfig.java**:

```java
package com.example.training.search.config;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.json.jackson.JacksonJsonpMapper;
import co.elastic.clients.transport.rest_client.RestClientTransport;
import org.apache.http.HttpHost;
import org.elasticsearch.client.RestClient;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ElasticsearchConfig {

    @Value("${elasticsearch.host:localhost}")
    private String host;

    @Value("${elasticsearch.port:9200}")
    private int port;

    @Bean
    public RestClient restClient() {
        return RestClient.builder(new HttpHost(host, port)).build();
    }

    @Bean
    public ElasticsearchClient elasticsearchClient(RestClient restClient) {
        RestClientTransport transport = new RestClientTransport(
            restClient,
            new JacksonJsonpMapper()
        );
        return new ElasticsearchClient(transport);
    }
}
```

### 1.4 Product 도메인

**Product.java** (JPA Entity):

```java
package com.example.training.product.domain;

import jakarta.persistence.*;

@Entity
@Table(name = "products")
public class Product {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;

    @Column(nullable = false)
    private String name;

    @Column(columnDefinition = "TEXT")
    private String description;

    @Column(nullable = false)
    private String category;

    @Column(nullable = false)
    private String brand;

    @Column(nullable = false)
    private Integer price;

    @Column(nullable = false)
    private String status = "ACTIVE";  // ACTIVE, INACTIVE

    protected Product() {}

    public Product(String name, String description, String category, String brand, Integer price) {
        this.name = name;
        this.description = description;
        this.category = category;
        this.brand = brand;
        this.price = price;
    }

    // Getters
    public Long getId() { return id; }
    public String getName() { return name; }
    public String getDescription() { return description; }
    public String getCategory() { return category; }
    public String getBrand() { return brand; }
    public Integer getPrice() { return price; }
    public String getStatus() { return status; }
}
```

**ProductDocument.java** (ES Document):

```java
package com.example.training.product.search;

public record ProductDocument(
    Long id,
    String name,
    String description,
    String category,
    String brand,
    Integer price,
    String status
) {
    public static ProductDocument from(Product product) {
        return new ProductDocument(
            product.getId(),
            product.getName(),
            product.getDescription(),
            product.getCategory(),
            product.getBrand(),
            product.getPrice(),
            product.getStatus()
        );
    }
}
```

### 1.5 ProductService (DB + ES 동기화)

**ProductService.java**:

```java
package com.example.training.product.service;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch.core.IndexRequest;
import co.elastic.clients.elasticsearch.core.DeleteRequest;
import com.example.training.product.domain.Product;
import com.example.training.product.dto.CreateProductDto;
import com.example.training.product.repository.ProductRepository;
import com.example.training.product.search.ProductDocument;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import java.io.IOException;

@Service
public class ProductService {
    private static final String INDEX_NAME = "products";

    private final ProductRepository productRepository;
    private final ElasticsearchClient elasticsearchClient;

    public ProductService(ProductRepository productRepository,
                         ElasticsearchClient elasticsearchClient) {
        this.productRepository = productRepository;
        this.elasticsearchClient = elasticsearchClient;
    }

    @Transactional
    public Product create(CreateProductDto dto) {
        // 1. DB 저장
        Product product = new Product(
            dto.name(), dto.description(), dto.category(), dto.brand(), dto.price()
        );
        product = productRepository.save(product);

        // 2. ES 인덱싱
        indexProduct(product);

        return product;
    }

    @Transactional
    public void delete(Long id) {
        // 1. DB 삭제
        productRepository.deleteById(id);

        // 2. ES 삭제
        try {
            elasticsearchClient.delete(DeleteRequest.of(d -> d
                .index(INDEX_NAME)
                .id(id.toString())
            ));
        } catch (IOException e) {
            throw new RuntimeException("Failed to delete from Elasticsearch", e);
        }
    }

    private void indexProduct(Product product) {
        try {
            ProductDocument doc = ProductDocument.from(product);
            elasticsearchClient.index(IndexRequest.of(i -> i
                .index(INDEX_NAME)
                .id(product.getId().toString())
                .document(doc)
            ));
        } catch (IOException e) {
            throw new RuntimeException("Failed to index product", e);
        }
    }

    @Transactional(readOnly = true)
    public void reindexAll() {
        productRepository.findAll().forEach(this::indexProduct);
    }
}
```

### 1.6 Search API

**SearchService.java**:

```java
package com.example.training.search.service;

import co.elastic.clients.elasticsearch.ElasticsearchClient;
import co.elastic.clients.elasticsearch._types.query_dsl.Query;
import co.elastic.clients.elasticsearch.core.SearchRequest;
import co.elastic.clients.elasticsearch.core.SearchResponse;
import com.example.training.product.search.ProductDocument;
import com.example.training.search.dto.SearchProductsDto;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

@Service
public class SearchService {
    private final ElasticsearchClient elasticsearchClient;

    public SearchService(ElasticsearchClient elasticsearchClient) {
        this.elasticsearchClient = elasticsearchClient;
    }

    public List<ProductDocument> searchProducts(SearchProductsDto params) {
        List<Query> mustQueries = new ArrayList<>();

        // 키워드 검색
        if (params.q() != null && !params.q().isBlank()) {
            mustQueries.add(Query.of(q -> q
                .multiMatch(m -> m
                    .fields("name^2", "description")  // name에 2배 가중치
                    .query(params.q())
                )
            ));
        }

        // 필터
        List<Query> filterQueries = new ArrayList<>();
        if (params.category() != null) {
            filterQueries.add(Query.of(q -> q.term(t -> t.field("category").value(params.category()))));
        }
        if (params.brand() != null) {
            filterQueries.add(Query.of(q -> q.term(t -> t.field("brand").value(params.brand()))));
        }
        if (params.minPrice() != null || params.maxPrice() != null) {
            filterQueries.add(Query.of(q -> q.range(r -> {
                var range = r.field("price");
                if (params.minPrice() != null) range.gte(params.minPrice().doubleValue());
                if (params.maxPrice() != null) range.lte(params.maxPrice().doubleValue());
                return range;
            })));
        }

        try {
            SearchResponse<ProductDocument> response = elasticsearchClient.search(SearchRequest.of(s -> s
                .index("products")
                .from((params.page() - 1) * params.size())
                .size(params.size())
                .query(q -> q.bool(b -> {
                    if (!mustQueries.isEmpty()) b.must(mustQueries);
                    if (!filterQueries.isEmpty()) b.filter(filterQueries);
                    return b;
                }))
            ), ProductDocument.class);

            return response.hits().hits().stream()
                .map(hit -> hit.source())
                .toList();
        } catch (IOException e) {
            throw new RuntimeException("Search failed", e);
        }
    }
}
```

---

## 2. S2.5: Kafka Async Events

### 2.1 목표

* Kafka Producer/Consumer 구현
* 도메인 이벤트 발행/소비
* Event-driven architecture 경험

### 2.2 Docker Compose (Kafka)

**docker-compose.yml** (추가):

```yaml
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

**실행**:

```bash
docker-compose up -d zookeeper kafka
```

### 2.3 Spring Kafka 설정

**build.gradle**:

```gradle
dependencies {
    implementation 'org.springframework.kafka:spring-kafka'
}
```

**KafkaProducerConfig.java**:

```java
package com.example.training.kafka.config;

import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.DefaultKafkaProducerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.core.ProducerFactory;
import org.springframework.kafka.support.serializer.JsonSerializer;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaProducerConfig {

    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> config = new HashMap<>();
        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        return new DefaultKafkaProducerFactory<>(config);
    }

    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

### 2.4 Order 도메인 (Producer)

**OrderCreatedEvent.java**:

```java
package com.example.training.order.event;

import java.time.LocalDateTime;

public record OrderCreatedEvent(
    String eventId,
    String eventType,
    LocalDateTime timestamp,
    Long orderId,
    Long userId,
    Integer totalAmount
) {
    public static OrderCreatedEvent of(Long orderId, Long userId, Integer totalAmount) {
        return new OrderCreatedEvent(
            java.util.UUID.randomUUID().toString(),
            "ORDER_CREATED",
            LocalDateTime.now(),
            orderId,
            userId,
            totalAmount
        );
    }
}
```

**OrderService.java**:

```java
package com.example.training.order.service;

import com.example.training.order.domain.Order;
import com.example.training.order.dto.CreateOrderDto;
import com.example.training.order.event.OrderCreatedEvent;
import com.example.training.order.repository.OrderRepository;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

@Service
public class OrderService {
    private static final String TOPIC = "order-events";

    private final OrderRepository orderRepository;
    private final KafkaTemplate<String, Object> kafkaTemplate;

    public OrderService(OrderRepository orderRepository,
                       KafkaTemplate<String, Object> kafkaTemplate) {
        this.orderRepository = orderRepository;
        this.kafkaTemplate = kafkaTemplate;
    }

    @Transactional
    public Order create(CreateOrderDto dto, Long userId) {
        // 주문 생성
        Order order = new Order(userId, dto.totalAmount());
        order = orderRepository.save(order);

        // 이벤트 발행
        OrderCreatedEvent event = OrderCreatedEvent.of(order.getId(), userId, order.getTotalAmount());
        kafkaTemplate.send(TOPIC, event);

        return order;
    }
}
```

### 2.5 Notification Consumer

**KafkaConsumerConfig.java**:

```java
package com.example.training.kafka.config;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.support.serializer.JsonDeserializer;

import java.util.HashMap;
import java.util.Map;

@EnableKafka
@Configuration
public class KafkaConsumerConfig {

    @Value("${spring.kafka.bootstrap-servers:localhost:9092}")
    private String bootstrapServers;

    @Bean
    public ConsumerFactory<String, Object> consumerFactory() {
        Map<String, Object> config = new HashMap<>();
        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        config.put(ConsumerConfig.GROUP_ID_CONFIG, "notification-consumer-group");
        config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        config.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        return new DefaultKafkaConsumerFactory<>(config);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, Object> kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, Object> factory =
            new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        return factory;
    }
}
```

**NotificationEventListener.java**:

```java
package com.example.training.notification.listener;

import com.example.training.notification.service.NotificationService;
import com.example.training.order.event.OrderCreatedEvent;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

@Component
public class NotificationEventListener {
    private static final Logger logger = LoggerFactory.getLogger(NotificationEventListener.class);
    private final NotificationService notificationService;

    public NotificationEventListener(NotificationService notificationService) {
        this.notificationService = notificationService;
    }

    @KafkaListener(topics = "order-events", groupId = "notification-consumer-group")
    public void handleOrderEvent(OrderCreatedEvent event) {
        logger.info("Received order event: {}", event.eventId());

        switch (event.eventType()) {
            case "ORDER_CREATED":
                String message = String.format("주문이 생성되었습니다. 주문 번호: %d, 금액: %d원",
                    event.orderId(), event.totalAmount());
                notificationService.create(event.userId(), "ORDER_CREATED", message);
                break;
            default:
                logger.warn("Unknown event type: {}", event.eventType());
        }
    }
}
```

---

## 3. S2.6: Docker, PostgreSQL, Redis 인프라

### 3.1 목표

* H2 → PostgreSQL 전환 (프로덕션 DB)
* Simple → Redis 캐시 (분산 캐싱)
* Docker Compose 전체 스택 컨테이너화
* 클라우드 배포 준비

### 3.2 PostgreSQL 설정

**application-prod.yml**:

```yaml
spring:
  datasource:
    url: jdbc:postgresql://${DB_HOST:localhost}:${DB_PORT:5432}/${DB_NAME:training}
    username: ${DB_USER:app}
    password: ${DB_PASSWORD:app}
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: validate  # 프로덕션에서는 validate만
    show-sql: false
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: false
```

**build.gradle** (의존성 추가):

```gradle
dependencies {
    runtimeOnly 'org.postgresql:postgresql'
}
```

### 3.3 Redis 캐시 통합

**build.gradle**:

```gradle
dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-data-redis'
}
```

**CacheConfig.java** (Redis 전환):

```java
package com.example.training.common.config;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.cache.CacheManager;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.concurrent.ConcurrentMapCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.redis.cache.RedisCacheConfiguration;
import org.springframework.data.redis.cache.RedisCacheManager;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;
import org.springframework.data.redis.serializer.RedisSerializationContext;
import org.springframework.data.redis.serializer.StringRedisSerializer;

import java.time.Duration;

@Configuration
@EnableCaching
public class CacheConfig {
    private static final Logger logger = LoggerFactory.getLogger(CacheConfig.class);

    @Bean
    @Primary
    @ConditionalOnProperty(name = "spring.cache.type", havingValue = "redis")
    public CacheManager redisCacheManager(RedisConnectionFactory connectionFactory) {
        logger.info("Using Redis cache manager");

        RedisCacheConfiguration cacheConfig = RedisCacheConfiguration.defaultCacheConfig()
            .entryTtl(Duration.ofMinutes(5))
            .serializeKeysWith(
                RedisSerializationContext.SerializationPair.fromSerializer(new StringRedisSerializer())
            )
            .serializeValuesWith(
                RedisSerializationContext.SerializationPair.fromSerializer(
                    new GenericJackson2JsonRedisSerializer()
                )
            );

        return RedisCacheManager.builder(connectionFactory)
            .cacheDefaults(cacheConfig)
            .build();
    }

    @Bean
    @ConditionalOnProperty(name = "spring.cache.type", havingValue = "simple", matchIfMissing = true)
    public CacheManager simpleCacheManager() {
        logger.warn("Redis not configured. Using in-memory cache");
        return new ConcurrentMapCacheManager("popularIssues", "stats", "external");
    }
}
```

**application-prod.yml** (Redis 추가):

```yaml
spring:
  cache:
    type: redis
  data:
    redis:
      host: ${REDIS_HOST:localhost}
      port: ${REDIS_PORT:6379}
```

### 3.4 Docker Compose

**docker-compose.yml** (전체 스택):

```yaml
version: '3.8'

services:
  # Spring Boot 애플리케이션
  app:
    build: .
    container_name: web-phase1-5-spring
    depends_on:
      - db
      - redis
    environment:
      SPRING_PROFILES_ACTIVE: prod
      DB_HOST: db
      DB_PORT: 5432
      DB_NAME: training
      DB_USER: app
      DB_PASSWORD: app
      REDIS_HOST: redis
      REDIS_PORT: 6379
    ports:
      - '8080:8080'
    networks:
      - app-network

  # PostgreSQL 16
  db:
    image: postgres:16
    container_name: postgres
    environment:
      POSTGRES_USER: app
      POSTGRES_PASSWORD: app
      POSTGRES_DB: training
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app-network

  # Redis 7
  redis:
    image: redis:7
    container_name: redis
    ports:
      - '6379:6379'
    volumes:
      - redis_data:/data
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
```

**Dockerfile**:

```dockerfile
FROM gradle:8.5-jdk17-alpine AS build
WORKDIR /app
COPY build.gradle settings.gradle ./
COPY gradle ./gradle
RUN gradle dependencies --no-daemon || true
COPY src ./src
RUN gradle clean build -x test --no-daemon

FROM eclipse-temurin:17-jre-alpine
WORKDIR /app
COPY --from=build /app/build/libs/*.jar app.jar
ENV SPRING_PROFILES_ACTIVE=prod
ENV SERVER_PORT=8080
EXPOSE 8080
HEALTHCHECK --interval=30s --timeout=3s --start-period=40s --retries=3 \
  CMD wget --no-verbose --tries=1 --spider http://localhost:8080/api/health || exit 1
ENTRYPOINT ["java", "-jar", "/app/app.jar"]
```

### 3.5 실행 및 검증

**전체 스택 시작**:

```bash
# 모든 서비스 시작
docker-compose up -d

# 로그 확인
docker-compose logs -f app
```

**검증**:

```bash
# 헬스 체크
curl http://localhost:8080/api/health

# PostgreSQL 연결 확인
docker-compose exec db psql -U app -d training -c "\dt"

# Redis 연결 확인
docker-compose exec redis redis-cli KEYS '*'

# 인기 이슈 캐시 확인
curl http://localhost:8080/api/issues/popular
docker-compose exec redis redis-cli GET "popularIssues::v1"
```

### 3.6 환경별 설정 전략

**로컬 개발** (`.env` 없이):
- `spring.profiles.active=local`
- H2 인메모리 DB
- Simple 인메모리 캐시

**Docker 환경** (docker-compose.yml):
- `spring.profiles.active=prod`
- PostgreSQL 16
- Redis 7

**프로덕션** (클라우드):
- 환경 변수로 주입
- 관리형 DB (RDS, Cloud SQL)
- 관리형 Redis (ElastiCache, Memorystore)

---

## 4. S2.4-S2.6 체크리스트

이 문서 기준으로, 아래를 만족하면 Stage 2 후반부 + Stage 3 (S2.4-S2.6)는 통과로 본다.

### S2.4 체크리스트 (Elasticsearch)

* [ ] Elasticsearch Java Client를 Spring Boot에 통합하고, 인덱스를 생성할 수 있다.
* [ ] 상품 생성/수정 시 DB와 ES를 동기화할 수 있다.
* [ ] 전문 검색 API를 구현하고, 키워드/필터로 검색 결과를 반환할 수 있다.
* [ ] 전체 재색인 엔드포인트를 만들고, DB 데이터를 ES에 일괄 동기화할 수 있다.

### S2.5 체크리스트 (Kafka)

* [ ] Kafka Producer를 구현하고, 도메인 이벤트를 발행할 수 있다.
* [ ] Kafka Consumer를 구현하고, 이벤트를 수신해서 처리할 수 있다.
* [ ] 주문 생성 → 이벤트 발행 → 알림 생성 플로우가 비동기로 동작하는 것을 확인할 수 있다.
* [ ] Consumer 중단/재시작 시에도 이벤트가 누락되지 않는 것을 확인할 수 있다.

### S2.6 체크리스트 (인프라)

* [ ] application-prod.yml에 PostgreSQL 설정을 추가하고 연결할 수 있다.
* [ ] Docker Compose로 PostgreSQL + Redis를 실행하고 애플리케이션이 연결되는 것을 확인할 수 있다.
* [ ] Redis 캐시가 동작하고, 캐시 히트 시 DB 조회가 발생하지 않는 것을 확인할 수 있다.
* [ ] Dockerfile을 작성하고 `docker-compose build`로 이미지를 빌드할 수 있다.
* [ ] 전체 스택을 `docker-compose up -d`로 실행하고 모든 서비스가 정상 동작하는 것을 확인할 수 있다.
* [ ] 로컬 개발(H2 + Simple), Docker(PostgreSQL + Redis) 환경을 모두 지원할 수 있다.

여기까지 구현하면:

* **Spring Boot 핵심 패턴** (Controller, Service, Repository, DI)
* **REST API** (CRUD, DTO validation, JWT)
* **RBAC** (@PreAuthorize)
* **배치 작업** (@Scheduled)
* **캐싱** (@Cacheable, Redis)
* **외부 API 연동** (타임아웃 + 재시도)
* **Elasticsearch 검색**
* **Kafka 이벤트 스트리밍**
* **프로덕션 인프라** (PostgreSQL, Redis, Docker)

까지 **실전 Spring Boot 백엔드 개발의 모든 핵심 패턴과 인프라 구축**을 경험한 상태다.

---

## 5. 참고 자료

* [Elasticsearch Java Client](https://www.elastic.co/guide/en/elasticsearch/client/java-api-client/current/index.html)
* [Spring Kafka Documentation](https://docs.spring.io/spring-kafka/reference/)
* [Spring Data Redis](https://docs.spring.io/spring-data/redis/reference/)
* [PostgreSQL Official Docs](https://www.postgresql.org/docs/)
* [Docker Compose Official Docs](https://docs.docker.com/compose/)
